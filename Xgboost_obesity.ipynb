{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFaLR681RR3rX5fq6bXFCi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuvrajDesh/data/blob/main/Xgboost_obesity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  XGBoost script\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xgboost as xgb\n",
        "import os\n",
        "\n",
        "SEED = 42\n",
        "VALID_SIZE = 0.20\n",
        "OUT_FILE = \"submission_xgb_earlystop.csv\"\n",
        "\n",
        "# Load data\n",
        "if not os.path.exists(\"train.csv\") or not os.path.exists(\"test.csv\"):\n",
        "    raise FileNotFoundError(\"Please put train.csv and test.csv in the working directory before running this script.\")\n",
        "\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df  = pd.read_csv(\"test.csv\")\n",
        "\n",
        "#  Preserve test ids\n",
        "if \"id\" in test_df.columns:\n",
        "    test_ids = test_df[\"id\"].copy()\n",
        "else:\n",
        "    test_ids = pd.Series(np.arange(len(test_df)), name=\"id\")\n",
        "\n",
        "# Drop id from train if present\n",
        "if \"id\" in train_df.columns:\n",
        "    train_df = train_df.drop(columns=[\"id\"])\n",
        "\n",
        "#  Separate target\n",
        "TARGET = \"WeightCategory\"\n",
        "if TARGET not in train_df.columns:\n",
        "    raise ValueError(f\"Target column '{TARGET}' not found in train.csv\")\n",
        "y_raw = train_df[TARGET].astype(str).copy()\n",
        "X_train_raw = train_df.drop(columns=[TARGET]).reset_index(drop=True)\n",
        "X_test_raw = test_df.copy()\n",
        "if \"id\" in X_test_raw.columns:\n",
        "    X_test_raw = X_test_raw.drop(columns=[\"id\"])\n",
        "X_test_raw = X_test_raw.reset_index(drop=True)\n",
        "\n",
        "#  Basic cleaning\n",
        "for df in (X_train_raw, X_test_raw):\n",
        "    for c in df.columns:\n",
        "        # numeric -> median\n",
        "        if df[c].dtype.kind in \"biufc\":\n",
        "            if df[c].isnull().any():\n",
        "                med = pd.concat([X_train_raw[c], X_test_raw[c]]).median()\n",
        "                df[c].fillna(med, inplace=True)\n",
        "        else:\n",
        "            # categorical/text -> string and placeholder for missing\n",
        "            df[c] = df[c].astype(str).fillna(\"NA\").str.strip()\n",
        "\n",
        "#  Frequency encoding for categorical columns\n",
        "combined_for_freq = pd.concat([X_train_raw, X_test_raw], axis=0, ignore_index=True)\n",
        "cat_cols = combined_for_freq.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "\n",
        "for col in cat_cols:\n",
        "    freq = combined_for_freq[col].value_counts(dropna=False)\n",
        "    mapping = freq.to_dict()\n",
        "    X_train_raw[col + \"_freq\"] = X_train_raw[col].map(mapping).fillna(0).astype(int)\n",
        "    X_test_raw[col + \"_freq\"]  = X_test_raw[col].map(mapping).fillna(0).astype(int)\n",
        "\n",
        "# drop original categorical columns\n",
        "X_train_raw = X_train_raw.drop(columns=cat_cols)\n",
        "X_test_raw  = X_test_raw.drop(columns=cat_cols)\n",
        "\n",
        "#  Align features (one-hot fallback then align)\n",
        "full = pd.concat([X_train_raw, X_test_raw], axis=0, ignore_index=True)\n",
        "remaining_cat = full.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "if remaining_cat:\n",
        "    full = pd.get_dummies(full, columns=remaining_cat, drop_first=False)\n",
        "\n",
        "X_all = full.iloc[: len(X_train_raw), :].copy()\n",
        "X_test = full.iloc[len(X_train_raw): , :].copy()\n",
        "X_all, X_test = X_all.align(X_test, join=\"left\", axis=1, fill_value=0)\n",
        "\n",
        "#  Encode target\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y_raw)\n",
        "num_classes = len(le.classes_)\n",
        "\n",
        "#  Train/validation split (stratified)\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "    X_all, y, test_size=VALID_SIZE, stratify=y, random_state=SEED\n",
        ")\n",
        "\n",
        "#  Set XGBoost params (many trees + small eta)\n",
        "params = {\n",
        "    \"objective\": \"multi:softprob\",\n",
        "    \"num_class\": num_classes,\n",
        "    \"eta\": 0.01,                 # small learning rate\n",
        "    \"max_depth\": 5,\n",
        "    \"subsample\": 0.8,\n",
        "    \"colsample_bytree\": 0.7,\n",
        "    \"min_child_weight\": 3,\n",
        "    \"alpha\": 0.01,              # L1 reg\n",
        "    \"lambda\": 1.0,              # L2 reg\n",
        "    \"eval_metric\": \"mlogloss\",\n",
        "    \"verbosity\": 0,\n",
        "    \"seed\": SEED\n",
        "}\n",
        "# Try to use 'hist' tree method when available for speed\n",
        "try:\n",
        "    params[\"tree_method\"] = \"hist\"\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "num_boost_round = 5000\n",
        "early_stopping_rounds = 100\n",
        "\n",
        "#  Build DMatrix with feature names preserved\n",
        "# xgboost will use DataFrame column names as feature names if we pass a DataFrame\n",
        "dtrain = xgb.DMatrix(X_tr, label=y_tr, feature_names=list(X_tr.columns))\n",
        "dval   = xgb.DMatrix(X_val, label=y_val, feature_names=list(X_val.columns))\n",
        "dall   = xgb.DMatrix(X_all, label=y, feature_names=list(X_all.columns))\n",
        "dtest  = xgb.DMatrix(X_test, feature_names=list(X_test.columns))\n",
        "\n",
        "watchlist = [(dtrain, \"train\"), (dval, \"valid\")]\n",
        "\n",
        "#  Train with early stopping\n",
        "t0 = time.time()\n",
        "bst = xgb.train(\n",
        "    params,\n",
        "    dtrain,\n",
        "    num_boost_round=num_boost_round,\n",
        "    evals=watchlist,\n",
        "    early_stopping_rounds=early_stopping_rounds,\n",
        "    verbose_eval=50\n",
        ")\n",
        "t_elapsed = time.time() - t0\n",
        "print(f\"\\nTraining completed in {t_elapsed:.1f}s. Best iteration: {bst.best_iteration}\")\n",
        "\n",
        "#  Validate\n",
        "pred_val = bst.predict(dval)\n",
        "pred_val_idx = np.argmax(pred_val, axis=1)\n",
        "val_acc = accuracy_score(y_val, pred_val_idx)\n",
        "print(\"Validation accuracy (after early stopping):\", val_acc)\n",
        "\n",
        "#  Retrain on full data using best_iteration\n",
        "best_iter = bst.best_iteration + 1 if bst.best_iteration is not None else num_boost_round\n",
        "print(\"Retraining on whole training set for\", best_iter, \"rounds...\")\n",
        "bst_full = xgb.train(params, dall, num_boost_round=best_iter, verbose_eval=50)\n",
        "\n",
        "#  Predict on test and save submission\n",
        "pred_test = bst_full.predict(dtest)\n",
        "pred_idx = np.argmax(pred_test, axis=1)\n",
        "pred_labels = le.inverse_transform(pred_idx)\n",
        "\n",
        "submission = pd.DataFrame({\"id\": test_ids, \"WeightCategory\": pred_labels})\n",
        "submission.to_csv(OUT_FILE, index=False)\n",
        "print(\"\\nSaved submission:\", OUT_FILE)\n",
        "print(submission.head())\n",
        "\n",
        "#  Feature importance (top 20)\n",
        "try:\n",
        "    importance = bst_full.get_score(importance_type=\"gain\")  # keys are feature names\n",
        "    if not importance:\n",
        "        print(\"No feature importance returned.\")\n",
        "    else:\n",
        "        fi = pd.Series(importance).sort_values(ascending=False)\n",
        "        fi = fi.rename_axis(\"feature\").reset_index(name=\"gain\")\n",
        "        # Show top 20\n",
        "        print(\"\\nTop 20 features by gain:\")\n",
        "        print(fi.head(20).to_string(index=False))\n",
        "except Exception as e:\n",
        "    print(\"Could not extract feature importance:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvV2jCkLiCxb",
        "outputId": "24c458e9-1a82-4264-e961-2f8e570a946e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-mlogloss:1.92357\tvalid-mlogloss:1.92392\n",
            "[50]\ttrain-mlogloss:1.24781\tvalid-mlogloss:1.26244\n",
            "[100]\ttrain-mlogloss:0.90665\tvalid-mlogloss:0.92810\n",
            "[150]\ttrain-mlogloss:0.70210\tvalid-mlogloss:0.72772\n",
            "[200]\ttrain-mlogloss:0.57196\tvalid-mlogloss:0.60114\n",
            "[250]\ttrain-mlogloss:0.48357\tvalid-mlogloss:0.51588\n",
            "[300]\ttrain-mlogloss:0.42076\tvalid-mlogloss:0.45561\n",
            "[350]\ttrain-mlogloss:0.37610\tvalid-mlogloss:0.41348\n",
            "[400]\ttrain-mlogloss:0.34362\tvalid-mlogloss:0.38365\n",
            "[450]\ttrain-mlogloss:0.31830\tvalid-mlogloss:0.36125\n",
            "[500]\ttrain-mlogloss:0.29762\tvalid-mlogloss:0.34342\n",
            "[550]\ttrain-mlogloss:0.28127\tvalid-mlogloss:0.33003\n",
            "[600]\ttrain-mlogloss:0.26765\tvalid-mlogloss:0.31932\n",
            "[650]\ttrain-mlogloss:0.25620\tvalid-mlogloss:0.31099\n",
            "[700]\ttrain-mlogloss:0.24625\tvalid-mlogloss:0.30421\n",
            "[750]\ttrain-mlogloss:0.23768\tvalid-mlogloss:0.29846\n",
            "[800]\ttrain-mlogloss:0.23025\tvalid-mlogloss:0.29384\n",
            "[850]\ttrain-mlogloss:0.22353\tvalid-mlogloss:0.28968\n",
            "[900]\ttrain-mlogloss:0.21751\tvalid-mlogloss:0.28633\n",
            "[950]\ttrain-mlogloss:0.21202\tvalid-mlogloss:0.28333\n",
            "[1000]\ttrain-mlogloss:0.20682\tvalid-mlogloss:0.28091\n",
            "[1050]\ttrain-mlogloss:0.20220\tvalid-mlogloss:0.27904\n",
            "[1100]\ttrain-mlogloss:0.19775\tvalid-mlogloss:0.27737\n",
            "[1150]\ttrain-mlogloss:0.19359\tvalid-mlogloss:0.27582\n",
            "[1200]\ttrain-mlogloss:0.18950\tvalid-mlogloss:0.27452\n",
            "[1250]\ttrain-mlogloss:0.18571\tvalid-mlogloss:0.27355\n",
            "[1300]\ttrain-mlogloss:0.18194\tvalid-mlogloss:0.27253\n",
            "[1350]\ttrain-mlogloss:0.17820\tvalid-mlogloss:0.27168\n",
            "[1400]\ttrain-mlogloss:0.17452\tvalid-mlogloss:0.27099\n",
            "[1450]\ttrain-mlogloss:0.17127\tvalid-mlogloss:0.27030\n",
            "[1500]\ttrain-mlogloss:0.16812\tvalid-mlogloss:0.26966\n",
            "[1550]\ttrain-mlogloss:0.16510\tvalid-mlogloss:0.26914\n",
            "[1600]\ttrain-mlogloss:0.16212\tvalid-mlogloss:0.26877\n",
            "[1650]\ttrain-mlogloss:0.15935\tvalid-mlogloss:0.26848\n",
            "[1700]\ttrain-mlogloss:0.15662\tvalid-mlogloss:0.26810\n",
            "[1750]\ttrain-mlogloss:0.15392\tvalid-mlogloss:0.26781\n",
            "[1800]\ttrain-mlogloss:0.15126\tvalid-mlogloss:0.26766\n",
            "[1850]\ttrain-mlogloss:0.14873\tvalid-mlogloss:0.26751\n",
            "[1900]\ttrain-mlogloss:0.14608\tvalid-mlogloss:0.26735\n",
            "[1950]\ttrain-mlogloss:0.14377\tvalid-mlogloss:0.26726\n",
            "[2000]\ttrain-mlogloss:0.14140\tvalid-mlogloss:0.26731\n",
            "[2050]\ttrain-mlogloss:0.13913\tvalid-mlogloss:0.26726\n",
            "[2055]\ttrain-mlogloss:0.13891\tvalid-mlogloss:0.26727\n",
            "\n",
            "Training completed in 44.9s. Best iteration: 1955\n",
            "Validation accuracy (after early stopping): 0.9069842291599614\n",
            "Retraining on whole training set for 1956 rounds...\n",
            "\n",
            "Saved submission: submission_xgb_earlystop.csv\n",
            "      id       WeightCategory\n",
            "0  15533     Obesity_Type_III\n",
            "1  15534   Overweight_Level_I\n",
            "2  15535  Overweight_Level_II\n",
            "3  15536      Obesity_Type_II\n",
            "4  15537        Normal_Weight\n",
            "\n",
            "Top 20 features by gain:\n",
            "                            feature      gain\n",
            "                        Gender_freq 29.731684\n",
            "                             Weight 26.513632\n",
            "                               FCVC 15.096104\n",
            "family_history_with_overweight_freq  6.815218\n",
            "                          CAEC_freq  5.102483\n",
            "                          FAVC_freq  4.913729\n",
            "                             Height  4.847258\n",
            "                                TUE  4.584535\n",
            "                                Age  4.519403\n",
            "                               CH2O  4.164415\n",
            "                          CALC_freq  3.892681\n",
            "                           SCC_freq  3.579771\n",
            "                                NCP  3.463296\n",
            "                        MTRANS_freq  2.420286\n",
            "                                FAF  2.304143\n",
            "                         SMOKE_freq  1.057930\n"
          ]
        }
      ]
    }
  ]
}